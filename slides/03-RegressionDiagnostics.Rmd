---
title: "Statistical Modeling"
author: "CH.3 - Regression Diagnostics"
date: "SS 2021 || Prof. Dr. Buchwitz"
output:
  beamer_presentation:
    fig_width: 8
    fig_height: 5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, size="scriptsize", 
                      fig.align = "center")
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

## Load Data
load("~/sciebo/courses/bchwtz-smod/nongit/RABE5.RData")
library(equatiomatic)
library(car)
library(olsrr)
library(GGally)
library(stargazer)
library(texreg)
```


# Organizational Information

## Course Contents

| Session   | Topic                   |
| --------- |:------------------------|
|  1        | Simple Linear Regression     | 
|  2        | Multiple Linear Regression   |
|  3        | Regression Diagnostics       |
|  4        | Qualitative Variables as Predictors |
|  5        | Transformation of Variables |
|  6        | Weighted Least Squares |
|  7        | Correlated Errors |
|  8        | Analysis of Collinear Data |
|  9        | Working with Collinear Data  |
| 10        | Variable Selection Procedures |
| 11        | Logistic Regression |
| 12        | Further Topics |

# Regression Diagnostics

## Introduction

- In this chapter we talk about the **standard regressions assumptions**, the consequences when violating them and how to detect violations so that we can focus on the remainder of the course on methods of how to correct or compensate for violations.
- When those assumptions are violated, the discussed and derived results for making inferences about the regression coefficients do not hold, which essentially means that **conclusions drawn on the corresponding models are wrong**.
- The majority of the discussed methods are **graphical methods** which means that they may be somewhat subjective here or there, which needs to be kept in mind when interpreting diagnostic plots.

## Overview

1. Assumptions about the form of the model.
2. Assumptions about the errors.
3. Assumptions about the predictors.
4. Assumptions about the observations.

\bigskip
\begin{alertblock}{}
The properties of the least squares estimators (BLUE) are based on the discussed assumptions!
\end{alertblock}


## Assumption 1: Model

- The model that relates $Y$ and $X_1, X_2, \ldots, X_p$ is assumed to be **linear in the regression parameters** $\beta_0, \beta_1, \ldots, \beta_p$ so that

$$ \text{Model:}\quad Y = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon$$
$$ \text{Observation:}\quad y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i, \; i=1,2,\ldots,n$$
\medskip

- This assumption is called the **linearity assumption**.
- In simple linear regression checking can be done using a scatterplot of $Y$ versus $X$. For multiple linear regression there are other plotting techniques which we will discuss.
- When the linearity assumptin does not hold, transforming the data may lead to linearity (Transformations are discussed at a later point).


## Assumption 2: Errors

- The errors $\epsilon_1,\epsilon_2,\ldots,\epsilon_n$ are assumed to be **independently and identically distributed** (iid) normal random variables each with mean zero and commonon variance $\sigma^2$. This implies:
  + **Normality Assumption:** The error $\epsilon_i, i=1,2,\ldots,n$ has a normal distribution. 
  + The errors $\epsilon_1,\epsilon_2,\ldots,\epsilon_n$ have mean zero.
  + **Constant Variance Assumption:** The errors have the same (but unknown) variance $\sigma^2$. When this assumtion does not hold we have the *heteroscedasticity problem*.
  + **Independent errors Assumption:** $\epsilon_1,\epsilon_2,\ldots,\epsilon_n$  are independent of each other (pairwise covariances are zero). Violations lead to the *autocorrelation problem*.

## Assumption 3: Predictors

- Thre are three assumptions for the predictor variables.
  + The predictor variables $X_1,X_2, \ldots, X_p$ are **nonrandom**. This means the values $x_{1j},x_{2,j},\ldots,x_{nj}$ with $j=1,2,\ldots,p$ are fixed (which is usually only fully satisfied under experimental conditions). In practice the results presented hold, but results are conditional on the data.
  + The values $x_{1j},x_{2,j},\ldots,x_{nj}$ are measured without error (which is hardly ever satisfied). In practice it is sufficient, when the measurement error is small compared to the random error $\epsilon_i$.
  + The predictor variables $X_1,X_2,\ldots,X_p$ arre assumend to be linaerly independent of each other. This assumtion guarantees the uniqueness of the lest squares solution. If this assumption is violated this is to refeered as the *collinearity problem*.

\bigskip
\begin{alertblock}{}
\small The first two assumptions cannot be checked and do not play a role in our analysis. They have to be kept in mind when collecting data.
\end{alertblock}

## Assumption 4: Observations

- All observations are equally reliable and have an approximately equal role in determining the regression results. This means that they are equally relied on when drawing conclusions.

\bigskip
\pause

### Conclusion
Small or minor violations of the underlying assumptions do not invalidate the inferences or conclusions drawn from the analysis. Gross validations, however, can seriously distort conlusions. **It is essential to investigate all signs of assumption validations by _always_ checking the structure of the residuals and the data patterns at least using graphs!**

## Residuals

- Analysing residuals is a simple and effective method for detecting model deficiencies in regression analysis. In most analysis it is probably the **most important** part of an analysis.
- Residual plots may lead to suggestiosn fo structure or point to informaiton in the data tha tmight be missed or overlooked. Those clues can lead to a better understanding (and possibly a better model) of the underlying process.
- Starting point for the analysis are the **ordinary** least squares residuals that
can be calculated after obtained the fitted values:

$$ \hat y_i = \hat\beta_0 + \hat\beta_1 x_{i1} + \hat\beta_2 x_{i2} + \ldots + \hat\beta_p x_{ip}$$
$$e_i = y_i - \hat y_i \quad\text{for}\quad i = 1,2,\ldots,n$$

## Residuals

- The fitted values can also be written as function of the predictor variables, where $p_{ij}$ only depends on the predictor variables (essentially values from the hat matrix $\bf P$).

$$ \hat y_i = p_{i1} y_1 + p_{i2} y_2 + \ldots + p_{in} y_n$$

- When $i=j$ the value $p_{ii}$ represents the weight (leverage) given to $y_i$ in determining the $i$-th fitted value $\hat y_i$. The $n$  **leverage values** $p_{11}, p_{22}, \ldots, p_{nn}$ can be obtained by

$$p_{ii}=\frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum (x_i - \bar x)^2}$$

\bigskip
\begin{alertblock}{}
\center
A high laverage value indicates some "extremeness" in $X$.
\end{alertblock}
 
## Residuals

- The *ordinary least squares residuals* $e_1, e_2, \ldots, e_p$ do not have **unequal variances** $Var(e_i)=\sigma^2(1-p_{ii})$. Analyzing requires **standardized residuals** by calculating

$$z_i = \frac{e_i}{\sigma\sqrt{1-p_{ii}}}$$

- This requires an unbiased estimate for the unknown standard deviation $\sigma$ of $\epsilon$ for which we have two unbiased estimates to choose from

$$\hat\sigma^2=\frac{\sum e_i^2}{n-p-1}=\frac{SSE}{n-p-1} \quad\text{with}\quad \hat\sigma^2_{(i)}=\frac{SSE_{(i)}}{(n-1)-p-1}=\frac{SSE_{(i)}}{n-p-2}$$

- $SSE_{(i)}$ is the sum of squared residuals when the $i$-th observation is left out so that the model is fitted using $n-1$ observations.

## Residuals

The choice of variance estimates results in two different types of residuals, although both are unbiased estimates.

:::{.block}
### Internally studentized residuals (using $\hat\sigma^2$)

$$r_i = \frac{e_i}{\hat\sigma\sqrt{1-p_{ii}}} \quad\text{with}\quad \hat\sigma^2=\frac{SSE}{n-p-1}$$
:::

:::{.block}
### Externally studentized residuals (using $\hat\sigma^2_{(i)}$)

$$r_i^* = \frac{e_i}{\hat\sigma_{(i)}\sqrt{1-p_{ii}}} \quad\text{with}\quad \hat\sigma^2_{(i)}=\frac{SSE_{(i)}}{n-p-2}$$

\medskip
\small\center Called *externally studentized* because $e_i$ is not involved in (external to) $\hat\sigma^2_{(i)}$.
:::

In practice the difference between $r_i$ and $r_i^*$ is small and both could be used, so the difference is ignored in the following notation.

## Graphical Methods

**Dimensionality:**

- One-dimensional graphs, inidcate the distribution of a particular variable (e.g. symmetry, skewedness) and allow identification of outliers.
- Two-dimensional graphs allow exploration of relationships (by pairing variables) and general patterns.

**Step in Model Selection Process:**

- Graphs **before** fitting a model, to e.g. correct data errors, seelect variables and preparation for model selection.
- Graphs **after** fitting a model to check assumptions and assessing the goodness of fit.

## Example: Hamiltons Data

```{r}
P103
```

## Graphs before fitting a Model

1. Boxplot
2. Histogram
3. Pairsplot

## Boxplot

```{r}
boxplot(P103)
```

## Histogram

```{r, fig.height=4.5}
hist(P103$X2, main = "", ylab = "HÃ¤ufigkeit", xlab = "")
rug(P103$X2)
box()
```

## Pairsplot

```{r, echo=F}
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- round(cor(x, y), digits=2)
  txt <- paste0("r = ", r)
  cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * abs(r))
}
```
```{r}
pairs(P103, upper.panel=panel.cor)
```

## Pairsplot 

```{r}
GGally::ggpairs(P103)
```

## Pairsplot

**Interpretation:**

- The pairwise correlation should always be interpreted in conjunction with the scatter plots.
  + The correlation coefficients only measures **linear** dependence.
  + The correlation coefficient is **non-robuts** and may be substantially influenced by few data points.
- The appearence of the scatter plot only serves as an indication of the results to be expected.
  + In *simple linear regression* the plot of $Y$ and $X$ is expected to **show a linear pattern**.
  + In mulitple liinear regression the scatter plots between $Y$ and each $X$ **may or may not** show a linear pattern. 

\bigskip
\begin{alertblock}{}
\center
\textbf{The abscence of a linear pattern does not invalidate the linear model!}
\end{alertblock}



## Graphs after fitting a Model

1. Graphs for cheking the linearity and normality assumptions
2. Graphs for the detection of ouliers and influential observations
3. Diagnostic plots for the effect of variables

## Model Fitting `lm()`

```{r, size="tiny"}
mod <- lm(Y ~ 1 + X1 + X2, data=P103)
summary(mod)
```


## Model Fitting `olsrr::ols_regress()`

```{r, size="tiny"}
mod <- ols_regress(Y ~ 1 + X1 + X2, data=P103)
mod
```

## Model Presentation: Stargazer

```{r, results='asis'}
mod <- lm(Y ~ 1 + X1 + X2, data=P103)
stargazer::stargazer(mod, header=F, single.row = T)
```

## Model Presentation: Texreg

```{r, results='asis'}
mod <- lm(Y ~ 1 + X1 + X2, data=P103)
texreg::texreg(mod)
```

## Linearity and Normality

- **Residual Histogram:** Under the assumptions the histogram should resemble a normal distribution with symmetric shape and most observations around the center and few observations in the tails.
- **QQ-Plot:** Plot of the residuals versus the normals scores, which are what we would expect to obtain if the residuals were taken from a normal distribution. Under normality this plot should resemble a (nearly) straight line.
- **Residuals vs. Predictors:** The residuals should be uncorrelated with each of the predictors. If the assumptions hold, the plot should be a random scatter of points. Any pattern indicates a violation of an assumption, which often can be fixed using transformations.
- **Residuals vs. Fitted:** The residuals should also be uncorrelated with the fitted values, therefore this plot should also be a random scatter of points.

## Histogram

```{r, fig.height=4.5}
hist(residuals(mod), density = )
```

## QQ-Plot (Standardized Residuals)

```{r, fig.height=4.5}
qqnorm(rstandard(mod))
qqline(rstandard(mod))
```

## Standardized Residuals vs. Predictors

```{r, fig.height=4.5}
par(mfrow=c(1,2))
plot(y=rstandard(mod),x=P103$X1)
plot(y=rstandard(mod),x=P103$X2)
```

## Standardized Residuals vs. fitted Values

```{r, fig.height=4}
par(mfrow=c(1,1))
plot(y=rstandard(mod), x=fitted(mod))
```

## Leverage, Influence and Outliers 

- We want to ensure that the model is not overly determined by one or a few observations. In multiple linear regression this cannot be simply detected graphically.
- When identifiying influential data points (points that drag the regression line in their direction) looking a residuals does not necessarily help.
- Influential points can be identified, when the regression coefficients (fitted values, $t$-Tests, etc) change heavily, when we **omit these points** while estimating the model.

## Outliers

- **Outliers in the Response Variable:** Observations with large standardized residuals are outliers in the response variable as they lie far from the fitted line (in $Y$ direction). Outliers indicate a model failure for these observations.
- **Outliers in the Predictors:** Outliers can also occur in the $X$-Space. The leverage values $p_{ii}$ allow to measure these discrepancies (based on distance from $\bar x$) and are called **high-leverage** points. It should  be checked if those points are also **influential** before they are treated.

\begin{alertblock}{}
Inspecting the residuals is necessary but not sufficient as high-leverage points (usually twice the average size of $(p+1)/n$)  usually have small residuals.
\end{alertblock}

## Example: River Data

```{r,fig.height=4}
mod <- lm(Nitrogen ~ 1 + ComIndl, data = P010)
plot(y=P010$Nitrogen, x=P010$ComIndl, ylab="Nitrogen Concentration (mg/l)",
     xlab="Commercial and Industrial Land Usage (%)")
abline(mod)
```

## Example: River Data

```{r,size="tiny"}
round(cbind(obs=P010$ComIndl,residuals=rstandard(mod), leverage=hatvalues(mod)),2)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
A small residual value is desireable and the standardized residuals show no outlier. However, the small residual is not due to a good fits, but due to high leverage of the observations for Neversink and Hackensack.
\end{alertblock}
\end{textblock}

## Example: River Data

```{r, fig.height=4}
par(mfrow=c(1,2))
plot(rstandard(mod), main="Index plot of standardized residuals")
plot(hatvalues(mod), main="Index plot fo leverage values")
```

## Example: River Data

```{r}
olsrr::ols_plot_resid_stand(mod)
```

## Influence

- The influence of an observations my be measured by the effects on the fit when iti is omitted from the data in the fitting process. 
- $\hat\beta_{0(i)}, \hat\beta_{1(i)}, \ldots, \hat\beta_{p(i)}$ denote the regression coefficients obtained when the $i$-th observation sis deleted. $\hat y_{1(i)}, \hat y_{2(i)}, \ldots, \hat y_{n(i)}$ and $\hat\sigma^2_{(i)}$ denote the predicted values and residual mean square error when dropping the $i$-th observation respectively. The resulting observation equation for the models follows by

$$ \hat y_{m(i)} = \hat\beta_{0(i)} + \hat\beta_{1(i)}x_{m1} + \ldots + \hat\beta_{p(i)}x_{mp}$$

- Measures to assess influence usually look at differences produced in $\hat\beta_j - \hat\beta_{j(i)}$ or $\hat y_j - \hat y_{j(i)}$.

## Cook's Distance

- *Cook's distance* measures the difference between the gregression coefficients obtained from the full data and the regression coefficients
obtained by deleting the $i$-th observation. 
- The influence of the $i$-th observation for $i = 1,\ldots,n$ is given by

$$ C_i = \frac{\sum_{j=1}^n (\hat y_j - \hat y_{j(i)})^2}{\hat\sigma^2(p+1)} = \frac{r_i^2}{p+1} \cdot \frac{p_{ii}}{1-p_{ii}}$$

\bigskip

- Cook's distance is a product of the sqaured residual and the so called **potential funciton** $p_{ii}/(1-p_{ii})$.
- If $C_i$ is large omitting a datapoint causes large changes in the model. Points are said to be influential when their Cook's distance meets or exceeds the 50% point of the $F$-distribution with $p+1$ and $n-p-1$ degrees of freedom. A rule of thumb is do investiage points where $C_i \geq 1$.

## Cook's Distance

```{r}
olsrr::ols_plot_cooksd_chart(mod)
```

## Welsch and Kuh Measure (DFITS)

- The measure proposed by Welsch and Kuh is the scaled difference between the $i$-th fitted value obtained from the full data and the $i$-th fitted value obtained by deleting the respective observation. 

$$DFITS_i = \frac{\hat y_i - \hat y_{i(i)}}{\sigma^2_{(i)} \sqrt{p_{ii}}} = r_i^* \sqrt{\frac{p_{ii}}{1-p_{ii}}}$$

\bigskip

- Points with $|DFITS_i| > 2\sqrt{(p+1)/(n-p-1)}$ are usually classified as influential.
- $C_i$ and $DFITS_i$ are functions of the residual and levrage values. It is often sufficient to inspect either the Cook's distance or $DFITS$.

## Welsch and Kuh Measure (DFITS)

```{r}
olsrr::ols_plot_dffits(mod)
```

## What to do with outliers?

- Outliers and influential observations should not routinely be deleted or automatically down-weighted.
- **Points with high leverage and high influence are not necessarily bad observations!** They can be an indication of model mispecification (e.g. non-linearity in the data) or show that the data did not come from a normal polulation. **In those cases they may be the most informative data points.**
- Each *outlier* should be inspected with care and checked individually. If data points are removed this should be documented in the research including reasons for the decision. 
- Antoher option is to use *robust regression* where less weight is given to data points with high leverage.

## Role of Variables in Regression Models

- Predictors are usually **sequentially introduced** into a regression equaiton. 
- Given a model that contains $p$ predictors, what is the effect of deleting (or adding) one of the variables from (or to) the model?
- One indication can be obtained by the $t$-test. If the $t$-value is large in absolute terms, the variables retained, otherwise omitted.
- **The results of the $t$-test are only valid if the underlying assumptions hold, tehrefore additinal graphs should be inspected when deciding wether or not to include a variable in the regresison model. 

## Example: Scottish Hills Races Data
```{r, size="tiny"}
P120
```

## Example: Scottish Hills Races Data
```{r}
lapply(P120, summary)
```

## Example: Scottish Hills Races Data
```{r, size="tiny"}
mod <- lm(Time ~ 1 + Distance + Climb, data=P120)
summary(mod)
```

## Added-Variable Plot

- The added-variable plot enables us to see the magnitude of the regression coefficient of the new variable that is bein considered for inclusion.
- The slope of the least squares line representing the points int he plot is equal to the estimated regression coefficient of the new variable. Additionally the plot shows data points that may be influential for this magnitude.

## Added-Variable Plot

- The added variable plot is a plot between two pairs of residuals: the residuals when $Y$ is regressed on predictors except $X_j$ versus the residuals when regressing $X_j$ on all other predictors.
  + First set of residuals corresponds to the remainder of $Y$ that can not be explained by the other regressors.
  + Second set of residuals corresponds to the part of $X_j$ that cannot be explained by the other predcitors. 
  + The resulting slope is equal to $\hat\beta_j$ and essentially a visualization that is equivalent to the interpretation as partial regression coefficient.

## Added-Variable Plot

```{r, fig.height=5, warning=F, message=F}
ols_plot_added_variable(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
Do both variables contribute significantly to the model? Is there anything unuasual that should be investigated further?
\end{alertblock}
\end{textblock}

## Effects of an additional Predictor

- When a new regressor is introduced two questions should be answerd:
  + Is the regression coefficient of the new variable significant (different from zero)?
  + Does the introduction fo the new variable substantially change the regressions coefficients that are already in the model.

## Effects of an additional Predictor

- **Option A:** insignificant and almost no change in coefficients $\rightarrow$ Should **not** be included unless theory dictates inclusion.
- **Option B:** significant and subtantial changes in coefficients. $\rightarrow$ Should be included, but needs to be checked for collinearity.
- **Option C:** significant but no substantial coefficient changes $\rightarrow$ Ideal condition as variable is uncorrelated with other regressors, variable should be retained.
- **Option D:** insignificant but substantial coefficient changes $\rightarrow$  indicates collinearity, corrective actions are reqauired before discussion.
