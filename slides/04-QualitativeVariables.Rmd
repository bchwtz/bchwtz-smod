---
title: "Statistical Modeling"
author: "CH.4 - Qualitative Variables"
date: "SS 2022 || Prof. Dr. Buchwitz"

toc: true
tocoverview: false
titlefontsize: 22pt

output: fhswf::presentation
knit: rmarkdown::render
---

```{r setup, include=FALSE}
library(fhswf)
library(knitr)
options(digits = 4)
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, 
                      dev.args=list(pointsize=11), size="scriptsize")

```


# Organizational Information

## Course Contents

| Session   | Topic                   |
| --------- |:------------------------|
|  1        | Simple Linear Regression     | 
|  2        | Multiple Linear Regression   |
|  3        | Regression Diagnostics       |
|  4        | Qualitative Variables as Predictors |
|  5        | Transformation of Variables |
|  6        | Weighted Least Squares |
|  7        | Correlated Errors |
|  8        | Analysis of Collinear Data |
|  9        | Working with Collinear Data  |
| 10        | Variable Selection Procedures |
| 11        | Logistic Regression |
| 12        | Further Topics |

# Qualitative Variables as Predictors

## Introduction

- Qualitative or categorical variables (such as gender, marital status, etc.) are useful predictors and are usually called **indicator** or **dummy variables**.
- Those variables usually only take two values, 0 and 1, which signify that the observation belongs to one of two possible categories.
- The numerical values of indicator variables **do not reflect quantitative ordering**.
- **Example Variable:** Gender, coded as 1 for *female* and 0 for *male*.
- Indicator variables can also be used in a regression equation to distinguish between three or more groups.
- The response variable is stil a quantiative continuous in all discussed cases.

## Example: Salary Survey Data

```{r, size="tiny"}
P130
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
\small
Salary survey of computer professionals with objective to identify and quantify variables that determine salary differentials.\\
$S$      Salary (Response) \\
$X$      Experience, measured in years\\
$E$      Education, 1 (High School/HS), 2 (Bachelor/BS), 3 (Advanced Degree/AD)\\
$M$      Management 1 (is Manager), 0 (no Management Responsibility)
\end{alertblock}
\end{textblock}

## Example: Salary Survey Data

- **Experience:** We assume linearity, which means that each additional year is worth a fixed salary increment. 
- **Education:** Can be used in a linear or categorial form.
  + Using the variable in its raw form would assume that each step up in education is worth a fixed increment in salary. This may be too restrictive.
  + Using education as categorical variable can be done by defining **two indicator variables**. This allows to pick up the effect of education wether it is linear or not.
- **Management:** Is also an indicator variable, that allows to distinguish between management (1) an regular staff positions (0).

## Indicator Variables

When using indicator variables to represent a set of categroies, the number of these variables required is **one less than the number of categories**. For *education* we can create two indicators variables:

$$
E_{i1}=
\begin{cases}
1, \text{if the i-th person is in the HS category}\\
0, \text{otherwise}.
\end{cases}
$$
\smallskip

$$
E_{i2}=
\begin{cases}
1, \quad \text{if the i-th person is in the BS category}\\
0, \quad \text{otherwise}.
\end{cases}
$$
\bigskip

These two variables allow representing the three groups (HS, BS, AD).
\center
HS: $E_1=1, E_2=0$, BS: $E_1=0,E_2=1$, AD: $E_1=0,E_2=0$

## Indicator Variables

- The regression equation from the Salary Survey Data is:

$$ S = \beta_0 + \beta_1 X + \gamma_1E_1 + \gamma_2E_2 + \delta_1M + \epsilon$$

\pause \bigskip

- There is a different valid regression equation for each of the six (three education and two managment) categories.

Category | $E$ | $M$ | Regression Equation
--- | --- | --- | ---
1 | 1 | 0 | $S=(\beta_0 + \gamma_1) \quad + \beta_1 X + \epsilon$
2 | 1 | 1 | $S=(\beta_0 + \gamma_1 + \delta_1) + \beta_1 X + \epsilon$
3 | 2 | 0 | $S=(\beta_0 + \gamma_2) \qquad + \beta_1 X + \epsilon$
4 | 2 | 1 | $S=(\beta_0 + \gamma_2 + \delta_1) + \beta_1 X + \epsilon$
5 | 3 | 0 | $S= \beta_0 \qquad\quad + \beta_1 X + \epsilon$
6 | 3 | 1 | $S=(\beta_0 + \delta_1) \quad + \beta_1 X + \epsilon$

## Indicator Variables

```{r, size="tiny"}
d <- P130
d$E1 <- as.numeric(d$E == 1)
d$E2 <- as.numeric(d$E == 2)
mod <- lm(S ~ 1 + X + E1 + E2 + M, data=d)
summary(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
Interpret the regression coefficients. Assume that the residual patterns are satisfactory. 
\end{alertblock}
\end{textblock}

## Model Comparison

```{r, results='asis', echo=F}
mod1 <- lm(S ~ 1 + X + E1 + E2 + M, data=d)
mod2 <- lm(S ~ 1 + X + E       + M, data=d)
stargazer::stargazer(mod1, mod2, header=F, single.row = T)
```


## Regression Diagnostics

**Before we continue we check the residuals**

1) Residuals vs. Years of Experience
2) Residuals vs. Categories from Dummys

## Regression Diagnostics

```{r, fig.height=4}
plot(x = d$X, y = rstandard(mod), pch=19,
     ylab="Residuals", xlab = "X",
     main = "Standardized Residuals vs. Years of Experience (X)")
```

## Regression Diagnostics

```{r, fig.height=4}
d$cat <- factor((paste0("E=",d$E,"&M=",d$M)))
plot(x = as.numeric(d$cat), y = rstandard(mod), pch=19, xaxt="n",
     ylab="Residuals", xlab = "Category",
     main = "Standardized Residuals vs. Education-Management Category")
axis(1,at=1:6,labels=levels(d$cat))
```

## Regression Diagnostics

**What is wrong with the residuals:**

- Depending on the category the residuals are almost entirely positive or negative.
- The **pattern of the residuals is highly moderated by the associated group** (education-management category). This makes it clear that the combinations of education and management have not been treated sufficiently in the model.
- The residual plots provide evidence that the effects of education and management status on salary determination are **not additive**.

\bigskip
\begin{alertblock}{}\large
The multiplicative pattern needs to be embedded in the model!
\end{alertblock}

## Interaction Effects

- Interaction effects are *multiplicative* effects that allow capturing nonadditive effects in variables.
- Interaction variables are products of existing indicator variables. 
- Using the Salary Survey Data this can be achieved by creating the two interaction effects $(E_1\cdot M)$ and $(E_2\cdot M)$ and **adding** them to the model.
- The interaction effects **do not replace** the indicator variables.

## Interaction Effects

```{r, size="tiny"}
mod <- lm(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
summary(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
Is that model sufficient?
\end{alertblock}
\end{textblock}

## Regression Diagnostics

```{r, size="tiny"}
summary(rstandard(mod))
```

```{r, echo=F, fig.height=4}
par(mfrow=c(1,2))
plot(x = d$X, y = rstandard(mod), pch=19,
     ylab="Residuals", xlab = "X",
     main = "Standardized Residuals vs. \n Years of Experience (X)")

plot(x = as.numeric(d$cat), y = rstandard(mod), pch=19, xaxt="n",
     ylab="Residuals", xlab = "Category",
     main = "Standardized Residuals vs. \n Education-Management Category")
axis(1,at=1:6,labels=levels(d$cat),cex.axis=0.5)
```

## Regression Diagnostics

```{r, size="tiny"}
d$res     <- residuals(mod)
d$res_std <- rstandard(mod)
tail(d, n=15)
d <- d[-33, ] # Remove problematic observation
```


## Interaction Effects

```{r, size="tiny", echo=F}
m <- olsrr::ols_regress(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
m <- as.vector(capture.output(m))
m <- m[!grepl("RMSE: Root Mean Square Error|MSE: Mean Square Error|MAE: Mean Absolute Error",m)]
m <- m[!(sapply(m,nchar) == 0)]
cat(m,sep="\n")
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny \textbf{Note:} The level accuracy with which the model explains the data is very rare! Usually Goodness of fit indicators are worse.
\end{alertblock}
\end{textblock}

## Interaction Effects

```{r, results="asis"}
mod <- lm(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
equatiomatic::extract_eq(mod, use_coefs=F, intercept="beta", wrap=T)
equatiomatic::extract_eq(mod, use_coefs=T, coef_digits=4, wrap=T)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny \textbf{Note:} The notation is slightly different here as the equations are automatically generated. However, it does not really matter whether you use a $\beta$, $\delta$ or any other greek letter for the (interaction) effects.
\end{alertblock}
\end{textblock}


## Interaction Effects

```{r}
# Data Preparation
d <- P130[-33, ]
d$cat <- factor((paste0("E=",d$E,"&M=",d$M)))
d$E.fac <- factor(d$E)

# Model estimation
mod1 <- lm(S ~ 1 + X + E.fac + M + E.fac*M, data=d)
mod2 <- lm(S ~ 1 + X + cat, data=d)
mod3 <- lm(S ~ 1 + X + E.fac*M, data=d)
```


\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your Turn}
\small Compare the models \texttt{mod1}, \texttt{mod2} and \texttt{mod3}. Use them to calculate the base salaries (no experience) for each of the six possible education-management categories.
\end{alertblock}
\end{textblock}


## Interaction Effects

```{r, echo=F}
df <- data.frame(X=0, E.fac=factor(rep(1:3,2)), M=rep(c(0,1),3))
basesalaries <- predict(mod1, newdata=df, interval = "confidence", level=0.95)

kable(cbind(id=1:6,df[,-1],basesalaries), align="c",
      col.names=c("Category", "E", "M", "Estimated Base Salary","95% CI Low","95% CI High"))
```


- All models lead to the **same estimates for the base salaries**. This shows that from a technical point using the `cat` variable (instead of the intercation effects) allows to capture the variation in the data.
- It is still **beneficial to use interaction effects** as we did, because this allows to separate the effects of the three sets of predictor variables education, management and education-management interaction.


## Systems of Regression Equations

A data set may consist of **two or more distinct subsets**, which may require individual regression equations to avoid bias. Subsets may occur cross-sectional or over time and need to be treated differently:
\bigskip

- Cross-Sectional Data
  1. Each group has a separate regression model.
  2. The models have the same intercept but different slopes.
  3. the models have the same slope but different intercepts.

- Time Series Data
  1. Calendar Effects, e.g. Seasonality
  2. Stability of regression parameters over time

## Example: Preemployment Test

```{r, size="tiny"}
P140
```


\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
\small
\texttt{TEST}\;      Score on the preemployment test.\\
\texttt{RACE}\;      Dummy to indicate if individual is part of a minority (1) or not (0).\\
\texttt{JPERF}\;     Job Performance Ranking after 6 weeks on the job.
\end{alertblock}
\end{textblock}

## Example: Preemployment Test

For simplicity and generality we refer to the job performance as $Y$ and the score on the preemployment test as $X$. We want to compare the following two models:

$$
\begin{array}{lcl}
\text{Model 1 (Pooled):} & \quad & y_{ij}=\beta_0+\beta_1x_{ij}+\epsilon_{ij}  \\
\text{Model 2 (Minority):} & \quad & y_{i1}=\beta_{01}+\beta_{11}x_{i1}+\epsilon_{i1} \\
\text{Model 2 (non Minority):} & \quad & y_{i2}=\beta_{02}+\beta_{12}x_{i2}+\epsilon_{i2}
\end{array}
$$

## Example: Preemployment Test

```{r, echo=F}
race <- P140[P140$RACE == 1, ]
white <- P140[P140$RACE == 0, ]
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)

mod1  <- lm(JPERF ~ 1 + TEST, data=P140)
mod2r <- lm(JPERF ~ 1 + TEST, data=race)
mod2w <- lm(JPERF ~ 1 + TEST, data=white)
mod3  <- lm(JPERF ~ 1 + TEST + RACE*TEST, data=P140)
  
abline(mod1, col="red")
abline(mod2r, col="black")
abline(mod2w, col="lightgrey")
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)

abline(mod1, col="red")
abline(mod2r, col="black")
abline(mod2w, col="lightgrey")

y <- 4.5
abline(h=y, col="blue", lty="dashed")
mod1_x <- (y - coef(mod1)[1])/coef(mod1)[2]
mod2r_x <- (y - coef(mod2r)[1])/coef(mod2r)[2]
mod2w_x <- (y - coef(mod2w)[1])/coef(mod2w)[2]

segments(mod1_x,0,mod1_x,y,col="blue",lty="dashed")
segments(mod2r_x,0,mod2r_x,y,col="blue",lty="dashed")
segments(mod2w_x,0,mod2w_x,y,col="blue",lty="dashed")
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny
When different models are required for the groups, this would imply that the required score in the preemployment test that is needed to result in (minimum) job performance (horizontal dashed line) needs to be distinguished by group (vertical dashed lines).
\end{alertblock}
\end{textblock}

## Models with different Slopes and different Intercepts

- What we want to test the Preemployment Test data for are differences in intercept and slope using the following Null.

$$H_0:\beta_{11}=\beta_{12},\beta_{01}=\beta_{02}$$

\medskip
- This test can be performed using an **interaction term** by using a variable $z_{ij}$ that takes the value 1 if an individual is part of a minority group and 0 otherwise. This leads to two relevant models:


$$
\begin{array}{lcl}
\text{Model 1 (Pooled):} & \quad & y_{ij}=\beta_0+\beta_1x_{ij}+\epsilon_{ij}  \\
\text{Model 3 (Interaction):} & \quad & y_{ij}=\beta_{0}+\beta_{1}x_{ij}+\gamma z_{ij}+\delta (z_{ij}\cdot x_{ij})+\epsilon_{ij} \\
\end{array}
$$

\medskip
- This model is **equivalent** to the previously discussed Model 2. 

## Models with different Slopes and different Intercepts

```{r, echo=F, results="asis"}
texreg::texreg(list(mod1,mod2r,mod2w,mod3),
               custom.model.names = c("Pooled","Minority","White","Interaction" ),
               custom.header=list("Model 1"=1,"Model 2"=2,"Model 2"=3,"Model 3"=4),
               table = T, caption = "")
```

- Model 1 can be seen as a restriced version (RM) of model 3,  the full model (FM), with $\gamma=\delta=0$.

## Models with different Slopes and different Intercepts

```{r, fig.height=4}
df <- cbind(P140,res = rstandard(mod3))
plot(x=df$TEST, y=df$res,
     ylab="Residuals", xlab="Preemployment Test Score")
points(df$TEST[df$RACE == T], df$res[df$RACE == T],pch=19)
```

## Models with different Slopes and different Intercepts

- The framework using the models as FM and RM allows us to use the $F$-Test for comparison.

$$F=\frac{[SSE(RM)-SSE(FM)]/2}{SSE(FM)/16}$$


```{r}
(SSE_RM <- sum(residuals(mod1)^2))
(SSE_FM <- sum(residuals(mod3)^2))
(F_stat <- ((SSE_RM - SSE_FM)/2)/(SSE_FM/16))
pf(F_stat, df1=2, df2=16, lower.tail=FALSE)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
\small
Interpret the F-Test.

Can you conclude that the relationship is different for the two groups, so that two different equations (intercept + slope) are required?
\end{alertblock}
\end{textblock}

## Models with same Slope and different Intercepts

- Assuming we have a reason to believe that only the intercepts for the two groups are different can be achieved using the indicator variable (and omitting the interaction term).

$$
\begin{array}{lcl}
\text{Model 1 (Pooled):} & \quad & y_{ij}=\beta_0+\beta_1x_{ij}+\epsilon_{ij}  \\
\text{Model 4 (Indicator only):} & \quad & y_{ij}=\beta_{0}+\beta_{1}x_{ij}+\gamma z_{ij}+ \cancel{\delta (z_{ij}\cdot x_{ij})}+\epsilon_{ij} \\
\end{array}
$$

- In the case where $z_{ij}=1$ (which indicates the non-minority group) the coefficient $\gamma$ can be added to the intercept $\beta_0$ to obtain the effective intercept for that respecitve group.

- The resulting models represent **two parallel lines** (same slopes) with intercepts $\beta_0$ and $\beta_0 + \gamma$.

## Models with same Slope and different Intercepts

```{r}
mod4  <- lm(JPERF ~ 1 + TEST + RACE, data=P140)
```

- Significance can be tested using the $F$-Test. As the FM and RM differ by one parameter, results are eqauivalent to the $t$-Test.

```{r, echo=F, results="asis"}
texreg::texreg(list(mod1,mod2r,mod2w,mod3,mod4),
               custom.model.names = c("Pooled","Minority","White","Interaction","Indicator" ),
               custom.header=list("Model 1"=1,"Model 2"=2,"Model 2"=3,"Model 3"=4,"Model 4"=5),
               table = T, caption = "")
```

## Models with different Slopes and same Intercept

- Finally we can hypothesize that the two groups have the same intercept $\beta_0$ but different slopes, which can be done by including only the interaction.

$$
\begin{array}{lcl}
\text{Model 1 (Pooled):} & \quad & y_{ij}=\beta_0+\beta_1x_{ij}+\epsilon_{ij}  \\
\text{Model 5 (Interaction only):} & \quad & y_{ij}=\beta_{0}+\beta_{1}x_{ij}+ \cancel{\gamma z_{ij}}+ \delta (z_{ij}\cdot x_{ij})+\epsilon_{ij} \\
\end{array}
$$

```{r}
mod5  <- lm(JPERF ~ 1 + TEST + RACE:TEST, data=P140)
```

- Inference for the $\delta$ can be carrioud out using the $F$-Test or the $t$-Test. The FM and RM again only differ by one parameter.

## Systems of Regeression Equations

- The final results for all discussed cases for the preemployment test data look like follows.

\bigskip 

```{r, echo=F, results="asis"}
texreg::texreg(list(mod1,mod2r,mod2w,mod3,mod4,mod5),
               custom.model.names = c("Pooled","Minority","White","Full Interaction","Indicator","Interaction"),
               custom.header=list("Model 1"=1,"Model 2"=2,"Model 2"=3,"Model 3"=4,"Model 4"=5,"Model 5"=6),
               table = T, caption = "")
```

## Time Series Data

- Another interesting field of study is temporal structure in the data, which could fill a whole course by itself. Therefore we only briefly look at two ideas.

1) **Calendar Effects, e.g. Seasonality**
  + Can be modeled by including time as regressor, e.g. in the form of (mulitple) indicators for e.g. Week/Month/Quarter/Year
  + The number of indicator variables is $m-1$ where $m$ is the frequency of the time effects (e.g. $m=4$ for Quarters).

2) **Stability of Parameters over Time**
  + By combining inidcator and interaction terms one can model intertemporal and interspatial relationships. Insignifcance of the interactions with all indicators then provices evidence stability over time.
