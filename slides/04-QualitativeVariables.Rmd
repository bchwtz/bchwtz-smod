---
title: "Statistical Modeling"
author: "CH.4 - Qualitative Variables"
date: "SS 2021 || Prof. Dr. Buchwitz"
output:
  beamer_presentation:
    fig_width: 8
    fig_height: 5
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, size="scriptsize", 
                      fig.align = "center")
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

## Load Data
load("~/sciebo/courses/bchwtz-smod/nongit/RABE5.RData")
library(equatiomatic)
library(car)
library(olsrr)
library(GGally)
library(stargazer)
library(texreg)
```


# Organizational Information

## Course Contents

| Session   | Topic                   |
| --------- |:------------------------|
|  1        | Simple Linear Regression     | 
|  2        | Multiple Linear Regression   |
|  3        | Regression Diagnostics       |
|  4        | Qualitative Variables as Predictors |
|  5        | Transformation of Variables |
|  6        | Weighted Least Squares |
|  7        | Correlated Errors |
|  8        | Analysis of Collinear Data |
|  9        | Working with Collinear Data  |
| 10        | Variable Selection Procedures |
| 11        | Logistic Regression |
| 12        | Further Topics |

# Qualitative Variables as Predictors

## Introduction

- Qualitative or categorical variables (such as gender, marital status, etc) are useful predictors and are usually called **indicator** or **dummy variables**.
- Those variables usually only take two values, 0 and 1, which signify that the observation belongs to one of two possible categories.
- The numerical values of indicator variables**do not reflect quantitative ordering**.
- **Example Variable:** Gender, coded as 1 for *female* and 0 for *male*.
- Indicator variables can also be used in a regression equation to distinguish between three or more groups.
- The response variable is stil a quantiative continuous in all discussed cases.

## Example: Salary Survey Data

```{r, size="tiny"}
P130
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
\small
Salary survey of computer professionals with objective to identify and quantify variables that determine salary differentials.\\
$S$      Salary (Response) \\
$X$      Experience, measured in years\\
$E$      Education, 1 (High School/HS), 2 (Bachelor/BS), 3 (Advanced Degree/AD)\\
$M$      Management 1 (is Manager), 0 (no Management Responsibility)
\end{alertblock}
\end{textblock}

## Example: Salary Survey Data

- **Experience:** We assume linearity, which means that each additional year is worth a fixed salary increment. 
- **Education:** Can be used in a linear or categorial form.
  + Using the the variable in its raw form would assume that each step up in education is worth a fixed increment in salary. This may be too restrictive.
  + Using education as categorical variable can be done by defining **two indicator variables**. This allows to pick up the effect of education wether it is linear or not.
- **Management:** Is also an indicator variable, that allows to distinguish between management (1) an regular staff positions (0).

## Indicator Variables

When using indicator variables to represent a set of categroies, the number of these variables required is **one less than the number of categories**. For *education* we can create two indicators variables:

$$
E_{i1}=
\begin{cases}
1, \text{if the i-th person is in the HS category}\\
0, \text{otherwise}.
\end{cases}
$$
\smallskip

$$
E_{i2}=
\begin{cases}
1, \quad \text{if the i-th person is in the BS category}\\
0, \quad \text{otherwise}.
\end{cases}
$$
\bigskip

These two variables allow representing the three groups (HS, BS, AD).
\center
HS: $E_1=1, E_2=0$, BS: $E_1=0,E_2=1$, AD: $E_1=0,E_2=0$

## Indicator Variables

- The regression equation from the Salary Survey Data is:

$$ S = \beta_0 + \beta_1 X + \gamma_1E_1 + \gamma_2E_2 + \delta_1M + \epsilon$$

\pause \bigskip

- There is a different valid regression equation for each of the six (three education and two managmeent) categories.

Category | $E$ | $M$ | Regression Equation
--- | --- | --- | ---
1 | 1 | 0 | $S=(\beta_0 + \gamma_1) \quad + \beta_1 X + \epsilon$
2 | 1 | 1 | $S=(\beta_0 + \gamma_1 + \delta_1) + \beta_1 X + \epsilon$
3 | 2 | 0 | $S=(\beta_0 + \gamma_2) \qquad + \beta_1 X + \epsilon$
4 | 2 | 1 | $S=(\beta_0 + \gamma_2 + \delta_1) + \beta_1 X + \epsilon$
5 | 3 | 0 | $S= \beta_0 \qquad\quad + \beta_1 X + \epsilon$
6 | 3 | 1 | $S=(\beta_0 + \delta_1) \quad + \beta_1 X + \epsilon$

## Indicator Variables

```{r, size="tiny"}
d <- P130
d$E1 <- as.numeric(d$E == 1)
d$E2 <- as.numeric(d$E == 2)
mod <- lm(S ~ 1 + X + E1 + E2 + M, data=d)
summary(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
Interpret the regression coefficients. Assume that the residual patterns are satisfactory. 
\end{alertblock}
\end{textblock}

## Model Comparison

```{r, results='asis', echo=F}
mod1 <- lm(S ~ 1 + X + E1 + E2 + M, data=d)
mod2 <- lm(S ~ 1 + X + E       + M, data=d)
stargazer::stargazer(mod1, mod2, header=F, single.row = T)
```


## Regression Diagnostics

**Before we continue we check the residuals**

1) Residuals vs. Years of Experience
2) Residuals vs. Categories from Dummys

## Regression Diagnostics

```{r, fig.height=4}
plot(x = d$X, y = rstandard(mod), pch=19,
     ylab="Residuals", xlab = "X",
     main = "Standardized Residuals vs. Years of Experience (X)")
```

## Regression Diagnostics

```{r, fig.height=4}
d$cat <- factor((paste0("E=",d$E,"&M=",d$M)))
plot(x = as.numeric(d$cat), y = rstandard(mod), pch=19, xaxt="n",
     ylab="Residuals", xlab = "Category",
     main = "Standardized Residuals vs. Education-Management Category")
axis(1,at=1:6,labels=levels(d$cat))
```

## Regression Diagnostics

**What is wrong with the residuals:**

- Depending on the category the residuals are almost entirely positive or negative.
- The **pattern of the residuals is highly moderated by the associated group** (education-management category). This makes it clear that thath the combinations of education and management have not been tretaed sufficiently in the model.
- The residual plots provide evidence that the effects of education and management status on salary determination are **not additive**.

\bigskip
\begin{alertblock}{}\large
The multiplicative pattern needs to be embedded in the model!
\end{alertblock}

## Interaction Effects

- Interaction effects are *multiplicative* effects that allow capturing nondditive effects in variables.
- Interaction variables are products of existing indicator variables. 
- Using the Salary Survey Data this can be achieved by creating the two interaction effects $(E_1\cdot M)$ and $(E_2\cdot M)$ and **adding** them to the model.
- The interaction effects **do not replace** the indicator variables.

## Interaction Effects

```{r, size="tiny"}
mod <- lm(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
summary(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
Is that model sufficient?
\end{alertblock}
\end{textblock}

## Regression Diagnostics

```{r, size="tiny"}
summary(rstandard(mod))
```

```{r, echo=F, fig.height=4}
par(mfrow=c(1,2))
plot(x = d$X, y = rstandard(mod), pch=19,
     ylab="Residuals", xlab = "X",
     main = "Standardized Residuals vs. \n Years of Experience (X)")

plot(x = as.numeric(d$cat), y = rstandard(mod), pch=19, xaxt="n",
     ylab="Residuals", xlab = "Category",
     main = "Standardized Residuals vs. \n Education-Management Category")
axis(1,at=1:6,labels=levels(d$cat),cex.axis=0.5)
```

## Regression Diagnostics

```{r, size="tiny"}
d$res     <- residuals(mod)
d$res_std <- rstandard(mod)
tail(d, n=15)
d <- d[-33, ] # Remove problematic observation
```


## Interaction Effects

```{r, size="tiny", echo=F}
m <- olsrr::ols_regress(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
m <- as.vector(capture.output(m))
m <- m[!grepl("RMSE: Root Mean Square Error|MSE: Mean Square Error|MAE: Mean Absolute Error",m)]
m <- m[!(sapply(m,nchar) == 0)]
cat(m,sep="\n")
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny \textbf{Note:} The level accuracy with which the model explains the data is very rare! Usually Goodness of fit indicators are worse.
\end{alertblock}
\end{textblock}

## Interaction Effects

```{r, results="asis"}
mod <- lm(S ~ 1 + X + E1 + E2 + M + E1*M + E2*M, data=d)
equatiomatic::extract_eq(mod, use_coefs=F, intercept="beta", wrap=T)
equatiomatic::extract_eq(mod, use_coefs=T, coef_digits=4, wrap=T)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny \textbf{Note:} The notation is slightly different here as the equations are automatically generated. However, it does not really matter wether you use a $\beta$, $\delta$ or any other greek letter for the (interaction) effects.
\end{alertblock}
\end{textblock}


## Interaction Effects

```{r}
# Data Preparation
d <- P130[-33, ]
d$cat <- factor((paste0("E=",d$E,"&M=",d$M)))
d$E.fac <- factor(d$E)

# Model estimation
mod1 <- lm(S ~ 1 + X + E.fac + M + E.fac*M, data=d)
mod2 <- lm(S ~ 1 + X + cat, data=d)
mod3 <- lm(S ~ 1 + X + E.fac*M, data=d)
```


\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your Turn}
\small Compare the models \texttt{mod1}, \texttt{mod2} and \texttt{mod3}. Use them to calculate the base salaries (no experience) for each of the six possible education-management categories.
\end{alertblock}
\end{textblock}


## Interaction Effects

```{r, echo=F}
df <- data.frame(X=0, E.fac=factor(rep(1:3,2)), M=rep(c(0,1),3))
basesalaries <- predict(mod1, newdata=df, interval = "confidence", level=0.95)

kable(cbind(id=1:6,df[,-1],basesalaries), align="c",
      col.names=c("Category", "E", "M", "Estimated Base Salary","95% CI Low","95% CI High"))
```


- All models lead to the **same estimates for the base salaries**. This shows that from a technical point using the `cat` variable (instead of the intercation effects) allows to capture the variation in the data.
- It is still **beneficial to use interaction effects** as we did, because this allows to seperate the effects of the three sets of predictor variables education, managemengt and education-management interaction.


## Systems of Regression Equations

A dataset may consists of **two or more distinct subsets**, which may require individual regression quations to avoid bias. Subsets may occure cross-sectional or over time and need to be treated differently:
\bigskip

- Cross-Sectional Data
  1. Each group has a separate regression model.
  2. The models have the same intercept but different slopes.
  3. the models have the same slope but different intercepts.

- Time Series Data
  1. Seasonality
  2. Stability of regression parameters over time

## Example: Preemployment Test

```{r, size="tiny"}
P140
```


\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your turn}
\small
\texttt{TEST}\;      Score on the preemployment test.\\
\texttt{RACE}\;      Dummy to indicate if individual is part of a minority (1) or not (0).\\
\texttt{JPERF}\;     Job Performance Ranking after 6 weeks on the job.
\end{alertblock}
\end{textblock}

## Example: Preemployment Test

For simplicity and generality we refer to the job performance as $Y$ and the score on the preemployment test as $X$. We want to compare the following two models:

$$
\begin{array}{lcl}
\text{Model 1 (Pooled):} & \quad & y_{ij}=\beta_0+\beta_1x_{ij}+\epsilon_{ij}  \\
\text{Model 2 (Minority):} & \quad & y_{i1}=\beta_{01}+\beta_{11}x_{i1}+\epsilon_{i1} \\
\text{Model 2 (non Minority):} & \quad & y_{i2}=\beta_{02}+\beta_1x_{12}+\epsilon_{i2}
\end{array}
$$

## Example: Preemployment Test

```{r, echo=F}
race <- P140[P140$RACE == 1, ]
white <- P140[P140$RACE == 0, ]
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)

mod1  <- lm(JPERF ~ 1 + TEST, data=P140)
mod2r <- lm(JPERF ~ 1 + TEST, data=race)
mod2w <- lm(JPERF ~ 1 + TEST, data=white)

abline(mod1, col="red")
abline(mod2r, col="black")
abline(mod2w, col="lightgrey")
```

## Example: Preemployment Test

```{r, echo=F}
plot(x=P140$TEST,y=P140$JPERF,
     ylab="Job Performance", xlab="Preemployment Test Score")

points(race$TEST,race$JPERF, pch=19)
#points(white$TEST,white$JPERF)

abline(mod1, col="red")
abline(mod2r, col="black")
abline(mod2w, col="lightgrey")

y <- 4.5
abline(h=y, col="blue", lty="dashed")
mod1_x <- (y - coef(mod1)[1])/coef(mod1)[2]
mod2r_x <- (y - coef(mod2r)[1])/coef(mod2r)[2]
mod2w_x <- (y - coef(mod2w)[1])/coef(mod2w)[2]

segments(mod1_x,0,mod1_x,y,col="blue",lty="dashed")
segments(mod2r_x,0,mod2r_x,y,col="blue",lty="dashed")
segments(mod2w_x,0,mod2w_x,y,col="blue",lty="dashed")
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{}
\tiny
When different models are required for the groups, this would imply that the required score in the preemployment test that is needed to result in (minimum) job performance needs to distinguished by group (vertical dashed lines).
\end{alertblock}
\end{textblock}


