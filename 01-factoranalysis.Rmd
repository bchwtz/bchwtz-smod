# Factor Analysis

Statistically speaking the main goal of factor analysis is to describe the covariance structure among many variables in terms of a few underlying, but unobservable random quantities, called factors. This usually happens by assuming that the supposed variables can be organized into (contextually meaningful) groups. The variables in a given group are assumed to be highly correlated and thus represent or are related to a latent construct. While the correlation within a group of variables is high, the correlation between different groups should be low. Following that argumentation it may be possible to condense the information from multiple observed variables within a group into a single unobserved factor variable. While Explanatory Factor Analysis (EFA) aims at finding the mentioned groups Confirmatory Factor Analysis (CFA) aims at confirming an *a priori* hypothesized variable grouping constellation.

While EFA and CFA serve different purposes when doing research, their foundation, especially the model formulation, the estimation and the derivation of quantities of interest is highly comparable and  will be presented below. 

## Foundations

$$
X_1 - \mu_1 = l_{11} F_1 + l_{12} F_2 + \ldots + l_{1m} F_m + \epsilon_1 \\
X_2 - \mu_2 = l_{21} F_1 + l_{22} F_2 + \ldots + l_{2m} F_m + \epsilon_2 \\
\vdots \\
X_p - \mu_p = l_{p1} F_1 + l_{p2} F_2 + \ldots + l_{pm} F_m + \epsilon_p
$$
Although the equations above seem to be related to multiple regression equations the fact that all quantities on the right hand side are unobseved (in fact only $X$ is observed) distinguishes this factor model from regression problems. The factor analysis model can also be written in matrix notation, which allows for easier derivation of some of the following procedures and thus should be considered valuable as well.

$$
\underset{p \times 1}{\boldsymbol X}  = 
\underset{p \times 1}{\boldsymbol \mu} +
\underset{p \times m}{\boldsymbol L} \; \underset{m \times p}{\boldsymbol F} +
\underset{p \times 1}{\boldsymbol \epsilon}
$$

$$
\underset{p \times 1}{\boldsymbol X - \boldsymbol \mu}  = 
\underset{p \times m}{\boldsymbol L} \; \underset{m \times p}{\boldsymbol F} +
\underset{p \times 1}{\boldsymbol \epsilon}
$$

$$
\boldsymbol \Sigma = Cov(\boldsymbol X) = 
\underset{\;\\Communality}{\boldsymbol L \boldsymbol L^T} + 
\underset{\;\\Uniqueness}{\boldsymbol \Psi}
$$

## Estimation

Work in Progress ...

### Principal Component Method

### Principal Factor Solution

### Maximum Likelihood Estimation


## Rotation

Work in Progress ...


## Basic Factor Analysis in R

### Data Preparation and Description

Lets get our hands dirty and apply what we learned. The following code loads some data that results from a questionnaire with 50 questions.

```{r}
q <- readRDS("data/fa_questions.rds")  # Questions asked
d <- readRDS("data/fa_sample.rds")     # Response data

#q <- readRDS(gzcon(url("https://bchwtz.de/lfs/fa_questions.rds")))
#q <- readRDS(gzcon(url("https://bchwtz.de/lfs/fa_sample.rds")))
```

The variable `q` contains the questions from the questionnaire (50 items) while `d` contains the actual responses. Respondents hat the option to answer each question using a 5-point Likert-Scale, that was labeled 1=Disagree, 3=Neutral, 5=Agree. However, the data is not pre-processed and needs to be thoroughly investigated and potentially cleaned up before it can be used for analytic procedures.

```{r}
head(q, n=6) # Show first 6 rows from the dataframe
```


The object `q` contains three variables (columns), the question ID `Q`, an indication if the question is positively (`coding == "P"`) or negatively (`coding == "N"`)  formulated and the actual formulation of the item `question`. The responses are saved in object `d` and the following code snipped shows the first six observations (rows) for the rist five variables `Q01` to `Q05` (columns).

```{r}
head(d[ ,1:5], n=6)
dim(d)         # Dimensions of the data matrix
```

<details>
  <summary>All 50 Items - Click to expand!</summary>
  
```{r}
library(kableExtra)
kable_paper(kbl(q), "hover", full_width = F)
```

</details>


In total we have `r nrow(d)` observations and `r ncol(d)` variables in the dataframe. The first 50 columns represent the responses to the items introduced above. The remaining three variables (`r tail(colnames(d),3)`) provide metadata and some technical information, which can be used to perform plausibility checks and cleanup the data.

- `IPC`: Is an abbreviation for IP (Internet Protocol Address) Count and lists the number of records from the user's IP address in the dataset. For max cleanliness is is recommended to only use records where this value is 1. It should, however, be noted that high values can have multiple causes and do no necessarily represent abusive usage of the survey service. They can be because of shared networks (e.g. entire universities) or multiple submissions.
- `testelapse`: Is the time in seconds spent on the page with the survey questions.
- `country`: Indicates the country of the respondet, but is determined by technical information and was *not asked as a question*, thus may be inaccurate.

```{r}
summary(d$testelapse)
```

The descriptive statistics for the `testelapse` variable shows that on average respondents spent `r round(mean(d$testelapse, na.rm=T),2)` seconds for answering the questions. 50% of the responses took longer than `r median(d$testelapse, na.rm=T)` seconds. However the fastest respondent only used the questionnaire for 2 seconds, while the longest response took `r max(d$testelapse, na.rm=T)` seconds ( = `r round(max(d$testelapse, na.rm=T)/60/60,2)` hours). Additionally `r sum(is.na(d$testelapse))` observations contain not value (`NA`) for the `testelapse` variable.

```{r}
table(d$IPC)
barplot(table(d$IPC))
```

When looking at the `IPC` variable using the raw numbers or the barplot above, we can see that most responses originated from a unique IP address, while there are fewer IPs that contributed multiple entries. However, there also seem to be some larger networks or abusive users that contribute multiple observations with the maximum being `r max(d$IPC, na.rm=T)` from a single IP.


The following output shows the descriptive statistics for the first five variables (`Q01`-`Q05`). As the responses were collected using a 5-point Likert-Scale the minimum for each variable is 1 and the maximum 5. The remaining values variate nicely and are all within a reasonable range. However, it becomes quickly visible that some respondents have not answered all questions as, each variable contains `NA` values.

```{r}
t(sapply(d[ , 1:5], summary))
```

<details>
  <summary>All 50 Descriptive Statistics - Click to expand!</summary>
  
```{r}
t(sapply(d[ , 1:50], summary))
```

</details>


Lets have a detailed look at `Q01`, which was formulated as shown below. Once again the scale meant the following: 1=Disagree, 3=Neutral, 5=Agree.

```{r}
q[q$Q == "Q01", ]
```
Deep diving into the descriptive statistics and actual responses for the variable `Q01` shows, that the value of the first quartile is 2, meaning that 25% or more of the respondents answered with either value 1 or 2 (the code below shows the frequencies and exact proportions). In total 32.62% disagree (at least lightly) with the item and do not feel comfortable around people. On the other side 39.27% somehow agree to the statement and have no worse feelings when being in a group of people. However, 28% of the respondents say that they feel neutral (no agreement or disagreement with the item) when in groups. Additionally 74 people have not answered `Q01`.
 
```{r}
tab <- as.vector(table(d[,1]))
q01_details <- data.frame(value = 1:5,
                          freq = tab,
                          prop = tab/sum(tab))
q01_details$cumprop <- cumsum(q01_details$prop)
round(q01_details, digits=4)
```

Before we start the analysis we are going to clean up the dataset based on the insights we gained in our descriptive analysis so far. The draw reliable conclusions we want the dataset as clean as possible, which is the rationale behind the following guidelines:

1) We want to ensure that respondents spent a reasonable amount of time when answering the questionnaire. We expect fast impulsive respondents to spent at least 2 seconds for every questions (thus 100 seconds in total) and not more than one hour for all questions (thus 3600 seconds).

2) We want to be sure that every natural person only contributes one observation to the dataset, which we will ensure by only considering lines where the `IPC` variable has the value `1`.

3) We do not want missing observations in the sense of unanswered questions (`NA` values) in the dataset, but only consider responses that answered all questions from the questionnaire.

```{r}
# 1) Remove rows with unreasonable response times
d_cleaned <- d[d$testelapse >= 100 & d$testelapse <= 3600, ]

# 2) Remove responses from non unique IPs
d_cleaned <- d[d$IPC == 1, ]

# 3) Remove rows with missing (NA) values.
d_cleaned <- na.omit(d_cleaned) 
```

Cleaning up our data naturally leads to observations that we do not consider. The dataset beforehand hat `r nrow(d)` observations, while our cleaned up dataset that is nor ready for analysis has only `r nrow(d_cleaned)` responses. While deleting `r nrow(d) - nrow(d_cleaned)`observations seems a lot, this is necessary to have a clean and logically flawless foundation for our analysis

```{r, collapse=T}
dim(d)                    # Dimensions for the raw data
dim(d_cleaned)            # Dimensions after cleaning up
nrow(d) - nrow(d_cleaned) # Number of discarded observations
```

While we could use the whole dataset for our analysis, 50 questions is a fairly long questionnaire and may be confusing for a first factor analysis. We therefore select only a subset of variables for further investigation. Specifically we revert to the following items and variables in the dataset. For the further analysis we also rely only on the responses. The metadata and technical information of the variables `IPC`, `testelapse` and `country` has been extensively for the descriptive statistics.

```{r}
items_selected <- c("Q01", "Q04", "Q11", "Q17", "Q28", "Q33")
d_small <- d_cleaned[ ,items_selected]
```

```{r}
q[q$Q %in% items_selected, ]
```

### Selecting numbers of Factors

Before applying a factor analysis routine we need to explicitly decide how many factors should be extracted from our cleaned and reduced dataset `d_small`. As factor analysis is essentially a elaborate decomposition of the variance-covariance-matrix of our variables, it is a good idea to look at the correlation matrix before proceeding.

**NOTE:** For the remainder of this chapter we treat the likert-scale responses as metric and calculate measures that were invented for continuous data (well knowing that this is a drawback with potential downsides).

```{r}
# Calculate Correlation Matrix, 
R <- cor(d_small)
round(R, digits=4)
```
As the correlation matrix is symmetric we can concentrate on the lower (or upper) triangle-matrix for our interpretation. 

However, the correlation structure does not provide a clear picture of the underlying constructs and it is still difficult to decide on the number of factors used. Therefore we consult additional measures that help us to decide. All the following statistics, the Kaiser-Guttman-Criterion, the criterion of extracted variance and the screeplot are based on the Eigenvalues of the correlation matrix that we calculated beforehand. While there are multiple functions (especially in the `psych` package) that allow for producing the results below more or less automatically, we quickly calculate the results ourselves to foster our understanding.

```{r}
# Eigenvalues & Proportion of Variance
eigval <- eigen(R)$values
eigval
```

First we need to calculate the Eigenvalue decomposition of the correlation matrix `R`, which can be done using the function `eigen()`. 

```{r}
# Scree plot
plot(eigval, type = "b", ylab = "Eigenvalue", xlab="Index",
     xaxt="n",ylim = c(0,max(eigval)),
     main = "Scree plot")

axis(1, at=1:length(eigval))
abline(h=1, lty = "dotted", col="red")
```

```{r}
proptotvar <- eigval / sum(eigval)
df <- data.frame(Eigenvalue = eigval, 
                 Var=proptotvar, 
                 sumVar=cumsum(proptotvar))
round(df, digits=4)
```

### Applying Factor Analysis

In R there are multiple implementations of factor analysis available. Each model variant and each implementation comes with its own strengths and drawbacks. A good starting point is the `factanal()` function that comes with the preinstalled `stats` package. However, more elaborate and flexible approaches are available for download (e.g. in the `psych` package). In this part we are focusing on the the factor analysis procedure that is implemented in the `factanal()` function, which performs  maximum-likelihood factor analysis on a covariance, correlation or data matrix. For using `factanal()` four arguments are essential `x` to input the data, `factors` to specify the number of factors that should be used, `scores` to specify the method used to calculate the factor scores and `rotation` to specify the method for orthogonal or oblique rotation.

```{r, eval=FALSE}
factanal(x,        # Data
         factors,  # Number of factors
         scores,   # Method to calculate factor scores
         rotation) # Method for rotation
```

While `x` and `factors` must be specified by the user the arguments `scores` and `rotation` use default values. Factor scores could be calculated either using Thompsons method (`scores = "regression"`), based on Bartlett's weighted least-squares approach (`scores = "Bartlett"`) or not calculated at all (defaul: `scores = "none"`). Applying factor rotation can either be done using the default Varimax option (`rotation = "varimax"`) for orthogonal transformation or using `rotation = "Bartlett"` for an oblique transformation. Alternatively results can be obtained without applying a transformation using `rotation = "none"`.

Based on our prior investigation we are now applying factor analysis with 2 factors. To get started we do not apply any rotational techniques, but start interpreting the results we obtain from that simple algorithm. This allows us to discuss the impact and effect of applying rotational transformations to the loading matrix in a subsequent step. As method for calculating the factor scores we apply the regression method. The results are estimated using Maximum Likelihood.

```{r}
fa_none <- factanal(d_small , 
                    factors = 2,
                    scores = "regression",
                    rotation = "none")
fa_none
```
The results for our initial factor analysis are stored in `fa_none`. The output above shows the following information:

- *Call:*

- *Uniqueness:*

- *Loadings:*

- *Explained Variance:*
    ```{r}
    # Calculating Sum of Squares (SS) loadings
    round(apply(fa_none$loadings^2,2,sum), digits=4)
    
    # Calculating Sum of Squares (SS) loadings by eigenvalue decomposition of squared loading matrix.
    round(eigen( L %*% t(L))$values, digits=4)
    ```
- *Likelihood Ratio Test:*


```{r}
L <- fa_none$loadings
Psi <- diag(fa_none$uniquenesses)
R_hat <- L %*% t(L) + Psi
```
    
    
```{r}
# Residual Matrix
round(R - R_hat, digits=4)
```

```{r}
fa_varimax <- factanal(d_small , factors = 2, rotation = "varimax")
fa_promax <- factanal(d_small, factors = 2, rotation = "promax")
```


```{r}
par(mfrow = c(1,3))
# par(mfrow = c(1,3), pty="s") # Force plots to be quadratic

plot(fa_none$loadings[,1], 
     fa_none$loadings[,2],
     xlab = "Factor 1", 
     ylab = "Factor 2", 
     ylim = c(-1,1),
     xlim = c(-1,1),
     main = "No rotation")
abline(h = 0, v = 0, col="darkgrey")

plot(fa_varimax$loadings[,1], 
     fa_varimax$loadings[,2],
     xlab = "Factor 1", 
     ylab = "Factor 2", 
     ylim = c(-1,1),
     xlim = c(-1,1),
     main = "Varimax rotation")

text(fa_varimax$loadings[,1]-0.08, 
     fa_varimax$loadings[,2]+0.08,
     colnames(d_small),
     col="blue")
abline(h = 0, v = 0, col="darkgrey")

rotmat <- fa_varimax$rotmat
segments(0,0,x1=rotmat[2,1],y1=rotmat[1,1], col="red")
segments(0,0,x1=rotmat[2,2],y1=rotmat[1,2], col="red")

plot(fa_promax$loadings[,1], 
     fa_promax$loadings[,2],
     xlab = "Factor 1", 
     ylab = "Factor 2",
     ylim = c(-1,1),
     xlim = c(-1,1),
     main = "Promax rotation")
abline(h = 0, v = 0, col="darkgrey")

rotmat <- fa_promax$rotmat
segments(0,0,x1=rotmat[2,1],y1=rotmat[1,1], col="red")
segments(0,0,x1=rotmat[2,2],y1=rotmat[1,2], col="red")


```

### Interpreting the Factors




```{r}

```


## Exercises {-}

```{r}
ski <- data.frame(skiers = paste0("S", c(1:5)),
                  cost = c(32, 61, 59, 36, 62),
                  lift = c(64, 37, 40, 62, 46),
                  depth = c(65, 62, 45, 34, 43),
                  powder = c(67, 65, 43, 35, 40))
```
